{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352de404",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction\n",
    "#### This project uses machine learning to predict the presence of heart disease based on various health metrics.\n",
    "In this project, I aim to build a binary classification MLP Neural Network to predict the presence of heart disease in patients based on various health metrics. The dataset used for this project is sourced from the UCI Machine Learning Repository and contains several features such as age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting ECG results, maximum heart rate achieved, and others. The project provides a comprehensive analysis of the dataset, including data preprocessing, model training, and evaluation. The final model is evaluated using accuracy, precision, recall, F1-score, and confusion matrix metrics to ensure its effectiveness in predicting heart disease.\n",
    "\n",
    "The final code cell provides an interactive interface for users to input their health metrics and receive a prediction on whether they are likely to have heart disease or not. This project serves as a practical application of machine learning techniques in the healthcare domain, demonstrating how data-driven approaches can aid in early detection and prevention of heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59647f14-c8c9-4929-8d4d-b519d4f83dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install torch kagglehub scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c97e9",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "There are 13 features in the dataset, and 1 target variable.\n",
    "### Heart Disease Dataset\n",
    "This dataset is used for predicting the presence of heart disease in individuals based on various medical attributes. The dataset contains both categorical and numerical features.\n",
    "### Features\n",
    "- **age**: Age of the individual\n",
    "- **sex**: Sex of the individual (1 = male; 0 = female)\n",
    "- **cp**: Chest pain type\n",
    "    - Value 1: typical angina\n",
    "    - Value 2: atypical angina\n",
    "    - Value 3: non-anginal pain\n",
    "    - Value 4: asymptomatic\n",
    "- **trestbps**: Resting blood pressure (in mm Hg)\n",
    "- **chol**: Serum cholesterol in mg/dl\n",
    "- **fbs**: Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "- **restecg**: Resting electrocardiographic results\n",
    "    - Value 0: normal\n",
    "    - Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "- **thalach**: Maximum heart rate achieved\n",
    "- **exang**: Exercise induced angina (1 = yes; 0 = no)\n",
    "- **oldpeak**: ST depression induced by exercise relative to rest\n",
    "- **slope**: Slope of the peak exercise ST segment\n",
    "    - Value 1: upsloping\n",
    "    - Value 2: flat\n",
    "    - Value 3: downsloping\n",
    "- **ca**: Number of major vessels (0-3) colored by fluoroscopy\n",
    "- **thal**: Thalassemia\n",
    "    - 3 = normal\n",
    "    - 6 = fixed defect\n",
    "    - 7 = reversible defect\n",
    "### Target Variable\n",
    "- **target**: Diagnosis of heart disease\n",
    "    - Value 0: No presence of heart disease\n",
    "    - Value 1: Presence of heart disease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a650d-30e3-4b24-a044-58112b5418fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies as needed\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Set the path to the file you'd like to load\n",
    "file_path = \"heart.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df: pd.DataFrame = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"johnsmith88/heart-disease-dataset\",\n",
    "  file_path,\n",
    ")\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5aa6ee",
   "metadata": {},
   "source": [
    "We import the dataset from kaggle (https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset) and load it into a pandas DataFrame. The dataset is then preprocessed to handle missing values and seperate the features and target variable. The features are standardized using `StandardScaler` from `sklearn.preprocessing`. The dataset is then split into training and testing sets using `train_test_split` from `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d845e9",
   "metadata": {},
   "source": [
    "## Formatting the pandas DataFrame into a PyTorch Dataset\n",
    "PyTorch requires the data to be in a specific format, so we create a custom dataset class that inherits from `torch.utils.data.Dataset`. This class will handle the loading and preprocessing of the data. The `__init__` method initializes the dataset, the `__len__` method returns the length of the dataset, and the `__getitem__` method retrieves a single sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee02162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom pytorch dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9992f",
   "metadata": {},
   "source": [
    "## Creating DataLoaders\n",
    "We use `torch.utils.data.DataLoader` to create DataLoaders for both the training and testing datasets. DataLoaders allow us to iterate over the dataset in batches, which is essential for training neural networks efficiently. We set the batch size to 32 and enable shuffling for the training DataLoader to ensure that the model sees the data in a different order each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de91884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert the tensors to a pytorch dataset\n",
    "train_dataset = HeartDiseaseDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = HeartDiseaseDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader: DataLoader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_dataloader: DataLoader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3604044",
   "metadata": {},
   "source": [
    "## Defining the Neural Network\n",
    "The goal is to create a neural network that can predict the presence of heart disease based on the features provided. We will use PyTorch to define and train this neural network. It will predict binary outcomes (presence or absence of heart disease) based on the input features.\n",
    "\n",
    "We define a simple feedforward neural network using `torch.nn.Module`. The network consists of an input layer, one hidden layer, and an output layer. The input layer has 13 neurons (corresponding to the 13 features), the hidden layer has 64 neurons with ReLU activation, and the output layer has 1 neuron that outputs the raw logits for the binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(13, 64),  # 13 input features\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 1),  # Output layer for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "# Initialize the model\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10a76f",
   "metadata": {},
   "source": [
    "# Implementing the training and testing loop functions\n",
    "\n",
    "## Training Loop\n",
    "### Abstract\n",
    "We implement the training loop to train the neural network on the training dataset. The training loop will iterate over the training DataLoader, compute the predictions, calculate the loss, and update the model parameters using backpropagation.\n",
    "\n",
    "### Detailed training steps\n",
    "On each iteration, we will:\n",
    "1. Forward pass the input data through the model to get the predictions.\n",
    "2. Compute the loss using binary cross-entropy with logits. The binary cross-entropy loss with logits is computed by combining a sigmoid activation function with the binary cross-entropy loss, which is suitable for binary classification tasks. The exact equation is: \n",
    "   $$\n",
    "   \\sigma(x) = \\frac{1}{1 + e^{-x}} \\\\\n",
    "   \\text{loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\cdot \\log(\\sigma(x_i)) + (1 - y_i) \\cdot \\log(1 - \\sigma(x_i))]\n",
    "   $$\n",
    "   where $y_i$ is the true label, $x_i$ is the model's output, and $\\sigma$ is the sigmoid function.\n",
    "3. Backward pass to compute gradients.\n",
    "4. Update the model parameters using the optimizer.\n",
    "5. Reset the gradients to zero for the next iteration. This is important to prevent accumulation of gradients from previous iterations, which can lead to incorrect updates.\n",
    "\n",
    "## Testing Loop\n",
    "### Abstract\n",
    "We implement the testing loop to evaluate the model's performance on the testing dataset. The testing loop will iterate over the testing DataLoader, compute the predictions, and calculate the accuracy of the model.\n",
    "\n",
    "### Detailed testing steps\n",
    "On each iteration, we will:\n",
    "1. Forward pass the input data through the model to get the predictions.\n",
    "2. Apply a sigmoid activation function to the output logits to convert them into probabilities.\n",
    "3. Convert the probabilities to binary predictions (0 or 1) based on a threshold (0.5).\n",
    "4. Compare the predictions with the true labels to compute the accuracy.\n",
    "5. Calculate precision, recall, and F1 score using `sklearn.metrics` for a more comprehensive evaluation of the model's performance and build a confusion matrix to visualize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd39f7",
   "metadata": {},
   "source": [
    "### Metrics used for Evaluation\n",
    "- **Accuracy**: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\n",
    "- **Precision**: The proportion of true positive results in all positive predictions made by the model.\n",
    "- **Recall**: The proportion of true positive results in all actual positive cases.\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "  - Harmonic Mean: The harmonic mean is a measure of the average of a set of numbers, calculated as the reciprocal of the arithmetic mean of the reciprocals of the numbers. It is particularly useful for rates and ratios, such as precision and recall in classification tasks.\n",
    "   $$\n",
    "   F1 = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n",
    "   $$\n",
    "- **Confusion Matrix**: A table used to describe the performance of a classification model, showing the true positive, true negative, false positive, and false negative counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ed6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "\n",
    "def train_loop(train_dataloader, model: NeuralNetwork, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X) # Forward pass through the model\n",
    "\n",
    "        # Unsqueeze to match the output shape\n",
    "        loss = loss_fn(pred, y.unsqueeze(1).float()) # Calculate BCE loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward() # Compute gradients\n",
    "        optimizer.step() # Make a step with the optimizer to update the model parameters\n",
    "        optimizer.zero_grad() # Reset the gradients to zero before the next iteration\n",
    "\n",
    "        loss, current = loss.item(), batch * BATCH_SIZE + len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(test_dataloader, model: NeuralNetwork, loss_fn: torch.nn.Module):\n",
    "    model.eval()\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.unsqueeze(1).float()).item()\n",
    "\n",
    "            # Convert logits to probabilities using sigmoid activation\n",
    "            pred = torch.sigmoid(pred)\n",
    "            predicted_classes = (pred > 0.5).float()\n",
    "\n",
    "            correct += (predicted_classes == y.unsqueeze(1)).sum().item()\n",
    "            all_preds.extend(predicted_classes.cpu().numpy())\n",
    "            all_labels.extend(y.unsqueeze(1).cpu().numpy())\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    # confusion matrix\n",
    "    confusion_matrix = sk_confusion_matrix(all_labels, all_preds)\n",
    "    confusion_df = pd.DataFrame(confusion_matrix, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_df)\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "        f\"Precision: {precision:>0.1f}, Recall: {recall:>0.1f}, F1 Score: {f1:>0.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5826463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, criterion, optimizer)\n",
    "    test_loop(test_dataloader, model, criterion)\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), \"heart_disease_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83397858",
   "metadata": {},
   "source": [
    "# Post-Training Thoughts\n",
    "After training the model for 10 epochs, we observe the following:\n",
    "- The model achieves an accuracy of 99% on the testing dataset.\n",
    "- The precision, recall, and F1 score are all exactly 1.0, indicating that the model performs well in distinguishing between the presence and absence of heart disease.\n",
    "- The confusion matrix does show that there are false negatives, which means that the model sometimes fails to identify individuals with heart disease, even though it performs well overall. However, there are no false positives, indicating that when the model predicts heart disease, it is almost always correct.\n",
    "\n",
    "Overall, the model demonstrates excellent performance on this dataset, achieving high accuracy and perfect precision, recall, and F1 score. The confusion matrix confirms that the model is effective in classifying heart disease cases, although there is room for improvement in reducing false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca938a",
   "metadata": {},
   "source": [
    "# Try it Yourself\n",
    "- Run the below cell to input your own data and see how the model performs.\n",
    "- You can input the values for each feature in the dataset, and the model will predict whether the individual has heart disease or not. It will also provide the probability of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_heart_disease(model, input_data):\n",
    "    model.eval()\n",
    "    input_tensor = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probability = torch.sigmoid(output).float()\n",
    "        prediction = 1 if probability > 0.5 else 0\n",
    "    return prediction, probability\n",
    "\n",
    "# Get user input for prediction\n",
    "def get_user_input():\n",
    "    print(\"Enter the following features for heart disease prediction:\")\n",
    "    features = []\n",
    "    features.append(float(input(\"Age: \")))\n",
    "    features.append(float(input(\"Sex (1 = male; 0 = female): \")))\n",
    "    features.append(float(input(\"Chest Pain Type (0-3): \")))\n",
    "    features.append(float(input(\"Resting Blood Pressure: \")))\n",
    "    features.append(float(input(\"Serum Cholesterol: \")))\n",
    "    features.append(float(input(\"Fasting Blood Sugar > 120 mg/dl (1 = true; 0 = false): \")))\n",
    "    features.append(float(input(\"Resting ECG (0-2): \")))\n",
    "    features.append(float(input(\"Max Heart Rate: \")))\n",
    "    features.append(float(input(\"Exercise Induced Angina (1 = yes; 0 = no): \")))\n",
    "    features.append(float(input(\"ST Depression: \")))\n",
    "    features.append(float(input(\"Slope of ST Segment (0-2): \")))\n",
    "    features.append(float(input(\"Number of Major Vessels (0-3): \")))\n",
    "    features.append(float(input(\"Thalassemia (1 = normal; 2 = fixed defect; 3 = reversable defect): \")))\n",
    "\n",
    "    features = np.array(features, dtype=np.float32)\n",
    "\n",
    "    # Standardize the input features\n",
    "    features = scaler.transform(features.reshape(1, -1)) # uses same scaler as training data\n",
    "\n",
    "    return features[0]\n",
    "\n",
    "# Interactive loop for predictions\n",
    "while True:\n",
    "    user_input = get_user_input()\n",
    "    prediction, probability = predict_heart_disease(model, user_input)\n",
    "    print(f\"Prediction: {'Heart Disease Likely' if prediction == 1 else 'Heart Disease Unlikely'}\")\n",
    "    print(f\"Probability: {probability.item():.4f}\")\n",
    "    \n",
    "    cont = input(\"Do you want to make another prediction? (yes/no): \").strip().lower()\n",
    "    if cont != 'yes':\n",
    "        break\n",
    "    print(\"Thank you for using the heart disease prediction model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ef193",
   "metadata": {},
   "source": [
    "Remember to run all the cells in order to ensure that the model is trained and ready for predictions!\n",
    "\n",
    "Thanks for reading! If you have any questions or suggestions, feel free to reach out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
